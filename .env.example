# Environment Variables for LocalLLM

# Override server host
# DEFAULT_HOST=0.0.0.0

# Override server port
# DEFAULT_PORT=8000

# Model storage directory
# MODEL_DIR=./models

# Device for inference (auto, cpu, cuda, mps)
# DEVICE=auto

# Maximum memory in GB
# MAX_MEMORY=8

# Hugging Face token (for private models)
# HUGGINGFACE_TOKEN=your_token_here

# Enable debug mode
# DEBUG=false

# Log level (DEBUG, INFO, WARNING, ERROR)
# LOG_LEVEL=INFO